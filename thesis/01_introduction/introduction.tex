\providecommand{\topdir}{..}
\documentclass[../main.tex]{subfiles}

\ifSubfilesClassLoaded{
    \externaldocument[main-]{../main}
    \setcounterref{chapter}{main-chap:introduction}
    \addtocounter{chapter}{-1}
}{}

\externaldocument{\subfix{../00_front_matter/front_matter}}
\externaldocument{\subfix{../02_lorenz96/lorenz96}}
\externaldocument{\subfix{../03_rayleigh_benard/rayleigh_benard}}
\externaldocument{\subfix{../04_tendencies/tendencies}}
\externaldocument{\subfix{../05_evaluation/evaluation}}
\externaldocument{\subfix{../06_conclusion/conclusion}}
\externaldocument{\subfix{../07_appendix/appendix}}

\begin{document}

\ifSubfilesClassLoaded{
    \frontmatter
    \tableofcontents
    \mainmatter
}{}

\chapter{Introduction} \label{chap:introduction}
\epigraphhead[0.1\textheight]{
    \epigraph{
        Bottomless wonders spring from simple rules which are repeated
        without end.
    }{
        \emph{Fractals and the Art of Roughness}\\
        Benoit Mandelbrot, 2010
    }
}

The Earth system (including atmosphere, ocean and land) is distinguished both
in its complexity and its influence on all terrestrial life. In particular, the
wellbeing of humanity is critically dependent on our ability to understand and
predict this system's behaviour---both short-term weather and long-term
climate. Our knowledge allows us to take action to reduce our vulnerability to
extreme events and long-term climate change, and limit anthropogenic impacts to
sustainable levels.

Our understanding of the Earth system is, to a large extent, derived from
numerical modelling of the fluid dynamics of the atmosphere and oceans on
finite-resolution grids. The achievable resolution is constrained by
computational cost; a typical atmospheric global climate model has a spatial
resolution on the order of $\ang{1}$ latitude/longitude, roughly corresponding
to the size of the entire Greater Sydney area from Katoomba to Bondi. Sub-grid
scale features, such as individual clouds, cannot be explicitly resolved, but
to ignore them completely would introduce unacceptable biases (inaccuracies,
relative to observations) in the model output. It is therefore necessary to
estimate the effects of these features in a process known as
\emph{parametrisation}. This introductory chapter will begin by presenting a
more rigorous description of the parametrisation problem in
\cref{sec:theory_background}. It will then review the practical implementation
of parametrisation schemes in weather and climate models, beginning with
traditional methods (\cref{sec:prac_background}) before introducing newer
techniques that attempt to remedy the limitations of the traditional methods
(\cref{sec:novel}).

These non-traditional parametrisation techniques themselves give rise to
several outstanding problems, motivating this thesis' focus on parametrisation
research that can be carried out in simpler dynamical system analogues of the
climate system, as a first step towards addressing the outstanding problems.
\cref{chap:lorenz96} will review the methods and findings of previous work in
the context of a very simple dynamical system known as Lorenz '96. In light of
the review, I will argue that future work should progress to parametrisation
for dynamical systems of intermediate complexity.

\cref{chap:rayleigh_benard} will introduce \rb{} convection, the chosen
intermediate-complexity test system for this work. The aim will be to construct
and assess a parametrisation for \rb{} convection using a \emph{data-driven}
approach analogous to those that have been attracting significant research
attention in the weather and climate modelling communities.
\cref{chap:tendencies} will formulate and implement a method for calculating
so-called subgrid tendencies---the effects of processes unresolvable by a
coarse modelâ€”--by systematically coarse-graining high-resolution training data.
A data-driven parametrisation scheme will be obtained by fitting a statistical
model to the subgrid tendency dataset. In \cref{chap:evaluation}, I will
couple the parametrisation scheme into a coarse \rb{} model to produce
an improved model that attempts to account for the effects of unresolved
processes, and determine whether the inclusion of the parametrisation
scheme improves the short-term forecast accuracy and long-term statistical
accuracy of the coarse model. Finally, \cref{chap:conclusion} will discuss
the findings of this work and draw final conclusions.


\section{Theoretical background}
\label{sec:theory_background}
Having introduced the parametrisation problem in informal language, I now treat
it more rigorously, first establishing its necessity from a mathematical
viewpoint in \cref{sec:necessity} and then formalising the seemingly vague
notion of ``effects of unresolved processes'' in \cref{sec:math}.

\subsection{The necessity of parametrisation} \label{sec:necessity}
% Broad introduction of weather/climate prediction and fluid equations
The primary task of general circulation models (GCMs) for Earth's weather and
climate is to simulate the dynamics of the atmosphere and ocean, which are
governed by the Navier-Stokes equations. The algorithm chosen to solve these
partial differential equations is known as a model's \emph{dynamical core}
\parencite{mcfarlane2011}. Since analytical solutions to the equations do not
exist, the dynamical core necessarily approximates the continuous equations
with finite-dimensional alternatives using one of several
possible discretisation schemes (e.g., the finite difference, finite element,
finite volume and spectral methods) \parencite{christensen2022}. In practice,
this usually involves representing the prognostic variables (i.e., those that
affect the evolution of the flow) with sets of discrete samples in space and
time, whose resolution is constrained by the available computing resources. The
unavoidable consequence of discretisation is the loss of information about
processes occurring on spatial and temporal scales smaller than the
corresponding sampling intervals. These processes are said to be
\emph{unresolved}.

% why we need parametrisation: nonlinearity
It is tempting to na\"{i}vely accept the loss of fine-scale information as a
necessary sacrifice and hope the dynamical core will still make accurate
predictions for the larger, resolved scales. Unfortunately, this too is
impossible due to the \emph{nonlinearity} of the governing equations. The
reason for this may be seen by considering linear differential equations as a
counterexample. If the governing equations were linear, they would allow
arbitrary superpositions of solutions, meaning that any given solution could be
partitioned into high- and low-frequency components, themselves also solutions.
One would thus have the freedom to solve for the low-frequency components alone
without compromise. This property may be understood more formally using the
Fourier transform, defined for a function $f$ of space and time by
\[
    \tilde{f}(\omega, \vec{k})
        = \int \d{t}\,\d^3{x}\, e^{i(\vec{k} \cdot \vec{x} - \omega t)}
        f(t, \vec{x}),
\]
which reduces any linear differential equation to an algebraic equation
relating the frequency $\omega$ to the wave vector $\vec{k}$. Each wavenumber
component of the initial state propagates trivially according to its own
time dependence $e^{-i(\vec{k} \cdot \vec{x} - \omega(\vec{k}) t)}$,
\emph{independently of the other components}. If the equations of fluid
dynamics were linear, one could safely neglect the fine-scale dynamics because
they would have no influence on the coarse scales. In reality, the equations
are nonlinear and cannot be solved in this way.

The consequence of the nonlinearity of the equations governing atmospheric and
oceanic flows is, therefore, a coupling of the resolved coarse scales to the
unresolved fine scales \parencite{mcfarlane2011}.
% justification via Reynolds averaging
This fact may be demonstrated more explicitly by applying so-called
\emph{Reynolds averaging} to the equations \parencite{christensen2022}.
Reynolds averaging decomposes each field $q$ into the sum of a coarse-grained
(in space or time) or statistical-ensemble-averaged field $\bar{q}$ and a
perturbation $q'$. Note that $\bar{q'} = 0$ by definition. The coarse-graining
operation is assumed to be linear, commute with differentiation and satisfy
$\bar{\bar{p} q} = \bar{p} \bar{q}$ for any two fields $p$ and $q$
\parencite{monin2007}. Following the example given by
\textcite{christensen2022}, consider the incompressible Navier-Stokes
equations, which have the general form
\begin{align*}
    \pdiff{u_i}{t} + u_j \pdiff{u_i}{x_j} &= \sum f_i, \\
    \pdiff{u_i}{x_i} &= 0
\end{align*}
where $u_i$ are the components of the velocity, $x_i$ are the
coordinates and $f_i$ are various forces per unit mass. Summation over repeated
indices is implied. Applying the decomposition and coarse-graining both sides
of the equations yields
\begin{alignat*}{2}
    && \pdiff{}{x_i} \left( \bar{u_i} + u_i' \right) &= 0 \\
    \Rightarrow & \qquad &
        \pdiff{\bar{u_i}}{x_i}
        + \underbrace{\pdiff{\bar{u_i'}}{x_i}}_{=0} &= 0 \\
    \Rightarrow & \qquad &
        \pdiff{\bar{u_i}}{x_i} = \pdiff{u_i'}{x_i} &= 0
\end{alignat*}
and
\begin{alignat*}{2}
    && \sum \bar{f_i} &=
        \bar{\pdiff{}{t} \left( \bar{u_i} + u_i' \right)}
        + \bar{\left( \bar{u_j} + u_j' \right)
        \pdiff{}{x_j} \left( \bar{u_i} + u_i' \right)} \\
    &&&=
        \pdiff{\bar{u_i}}{t}
        + \underbrace{\pdiff{\bar{u_i'}}{t}}_{=0}
        + \bar{u_j} \pdiff{\bar{u_i}}{x_j}
        + \bar{u_j} \underbrace{\pdiff{\bar{u_i'}}{x_j}}_{=0}
        + \underbrace{\bar{u_j'}}_{=0} \pdiff{\bar{u_i}}{x_j}
        + \bar{u_j' \pdiff{u_i'}{x_j}} \\
    &&&=
        \pdiff{\bar{u_i}}{t} + \bar{u_j} \pdiff{\bar{u_i}}{x_j}
        + \pdiff{\bar{u_i' u_j'}}{x_j}
        - \bar{u_i' \underbrace{\pdiff{u_j'}{x_j}}_{=0}} \\
    \Leftrightarrow & \qquad &
        \pdiff{\bar{u_i}}{t} + \bar{u_j} \pdiff{\bar{u_i}}{x_j}
        &= \sum \bar{f_i} - \pdiff{\bar{u_i' u_j'}}{x_j}.
\end{alignat*}
The physical meaning of this last equation is that the evolution of the
coarse-grained velocity field depends not only on itself and the coarse-grained
forces, but also on the perturbations via $\bar{u_i' u_j'}$, which does not
necessarily vanish. This dependence is a consequence of nonlinearity.

The theoretical discussion in this section establishes that physical processes
occurring at one place in the spectrum of temporal and spatial scales are
coupled to all the other processes in the spectrum \parencite{franzke2015}. In
particular, to ignore the effect of processes not explicitly resolved in
numerical models would introduce unacceptable systematic biases, or errors, in
the model forecasts. GCMs therefore require parametrisation schemes to estimate
the effects of unresolved processes as functions of the available large-scale
information (and the parameters of these functions must be chosen
appropriately---hence the name ``parametrisation'').


\subsection{Mathematical formulation of the parametrisation problem}%
\label{sec:math}
In order to make any progress on the parametrisation problem, it is necessary
to formalise the notion of estimating the ``effects'' of unresolved processes.
I will describe the general approach, which is well-established \parencite[see,
e.g.,][]{hasselmann1976,palmer2001,demaeyer2018,brajard2021}, while
acknowledging possible alternative conventions where they exist.

The Earth system may be considered as a dynamical system whose exact evolution
is governed by equations of the form
\begin{equation} \label{eqn:truth}
    \diff{\vec{z}}{t} = \vec{F}(\vec{z},t),
\end{equation}
where the vector $\vec{z}$ contains all the variables needed to fully specify
the state of the system and $\vec{F}$ is a nonlinear differential operator. One
then employs a change of variables $\vec{z} \to (\vec{x},\vec{y})$, where
$\vec{x}$ contains the information or variables that are explicitly resolved by
the model being studied and $\vec{y}$ contains the unresolved, `sub-grid'
information or variables. Mathematically, $\vec{x}$ and $\vec{y}$ are
projections of the full state $\vec{z}$ onto lower-dimensional subspaces of the
original state space \parencite{brajard2021}. Practically, $\vec{x}$ could be
obtained by averaging the original variables over several adjacent grid points,
with $\vec{y}$ being the corresponding residuals
\parencite{zacharuk2018,alcala2021}.

In principle, the change of variables splits \cref{eqn:truth}
into two parts
\begin{align*}
    \diff{\vec{x}}{t} &= \vec{G}(\vec{x},\vec{y},t), \\
    \diff{\vec{y}}{t} &= \vec{H}(\vec{x},\vec{y},t)
\end{align*}
that (still exactly) describe the coupled evolution of the resolved and
unresolved variables via nonlinear differential operators $\vec{G}$ and
$\vec{H}$. The goal of parametrisation is now to derive a new system of
equations
\begin{equation} \label{eqn:reduced_model}
    \diff{\tilde{\vec{x}}}{t} = \tilde{\vec{G}}(\tilde{\vec{x}},t)
\end{equation}
whose solution $\tilde{\vec{x}}(t)$ approximates the true $\vec{x}(t)$ as well
as possible without explicitly modelling $\vec{y}(t)$. The measure used to
compare $\tilde{\vec{x}}$ and $\vec{x}$ depends on the intended function of the
parametrised model; weather forecasting applications that prioritise short-term
predictive skill might call for minimisation of the root-mean-square error
(RMSE), while climate prediction would be more concerned with the closeness of
their probability distributions (including both mean and extreme values).

Before proceeding, it is important to note two assumptions that are implicitly
made in writing down \cref{eqn:reduced_model}. The first is that
$\tilde{\vec{G}}$ is a \emph{deterministic} function of $\tilde{\vec{x}}$. The
second is that $\tilde{\vec{G}}$ depends only on the value of $\tilde{\vec{x}}$
at time $t$ and not earlier times (that is to say, the parametrisation is
\emph{memoryless}). Recent research has shown that relaxing these assumptions
has the potential to greatly improve the accuracy and reliability of the
parametrised model, and this will be discussed in detail in
\cref{sec:novel,chap:lorenz96}. However, for the purpose of this basic
introduction, I will temporarily follow the traditional approach and retain
these assumptions.

There is more than one possible interpretation of $\tilde{\vec{G}}(\vec{x},t)$
and its relationship to $\vec{G}(\vec{x},\vec{y},t)$. \textcite{hasselmann1976}
describes $\tilde{\vec{G}}(\vec{x},t)$ as an ensemble average of
$\vec{G}(\vec{x},\vec{y},t)$, taken over the distribution of all $\vec{y}$ that
are possible for a given $\vec{x}$, i.e.
\begin{equation*}
    \tilde{\vec{G}}(\vec{x},t)
        = \langle \vec{G}(\vec{x},\vec{y},t) \rangle_{\vec{y}}.
\end{equation*}
\textcite{demaeyer2018} instead assert that the ultimate goal of
parametrisation is to literally approximate $\vec{y}$ as a function
$\vec{\xi}(\vec{x})$, in which case
\begin{equation*}
    \tilde{\vec{G}}(\vec{x},t)
        = \vec{G}(\vec{x},\vec{\xi}(\vec{x}),t).
\end{equation*}
In practice, $\vec{G}(\vec{x},\vec{y},t)$ usually separates into a known
resolved part $\vec{D}(\vec{x},t)$ independent of $\vec{y}$ and an unresolved
coupling term $\vec{C}(\vec{x},\vec{y},t)$. It then suffices to approximate
only the unresolved part by a parametrisation $\vec{P}(\vec{x}, t)$ using one
of the above methods, so that the parametrised model reads
\begin{equation} \label{eqn:parametrised_model}
    \diff{\tilde{\vec{x}}}{t}
        = \vec{D}(\tilde{\vec{x}},t) + \vec{P}(\tilde{\vec{x}}, t).
\end{equation}


\section{Practical background} \label{sec:prac_background}
Following the abstract theoretical argument in the previous sections, I will
next give a more practical introduction to the uses of parametrisation in
real weather and climate models (\cref{sec:phys_processes}) and the methods
that are traditionally used to construct parametrisation schemes
(\cref{sec:traditional}).

\subsection{Physical processes requiring parametrisation}
\label{sec:phys_processes}
I now turn to concrete examples of processes that are often represented by
parametrisations, restricting the discussion to atmospheric processes for
brevity. This section will give a basic introduction to each process, its
effect on the larger-scale behaviour of the climate system and the role of its
corresponding parametrisation scheme. The purpose of these examples is to
provide real-world context and motivation for current parametrisation research
in more idealised settings, which will form one of the main topics of this
review.

\emph{Cloud microphysics} parametrisations model the composition of clouds in
terms of the amount of water in the solid (e.g., hail), liquid (cloud droplets
and rain) and gaseous phases, and the rate of transitions between these phases.
Accurate representation of cloud formation and evolution is crucial for several
reasons. First, the interaction of clouds with solar and terrestrial radiation
has a major influence on the overall energy balance of the atmosphere
\parencite{mcfarlane2011}. Second, cloud water phase transitions lead to the
formation of precipitation, and are a source and sink of latent heat that
drives convection \parencite{mcfarlane2011}. Furthermore, the statistics of
cloud formation are linked to global temperatures in a feedback loop;
uncertainty in the sign and magnitude of this feedback effect is a major
contributor to uncertainty in the sensitivity of global temperatures to
increases in atmospheric CO$_2$ concentration \parencite{andrews2012,
christensen2022,stevens2013}. Microphysical processes occur on the spatial
scales of single water droplets and ice particles (of order
$\SIrange{e-6}{e-2}{\meter}$; \textcite{lamb2003}), among the smallest scales
in the atmosphere. All atmospheric models must parametrise the amount of cloud
water in each phase, the size distribution and concentration of liquid and ice
particles, and the resulting rate and type (rain, hail, snow, etc.) of
precipitation \parencite{christensen2022}.

\emph{Moist convection} encompasses vertical motions in the atmosphere that are
accompanied (and driven) by phase changes of water, and must be parametrised
when it occurs on sub-grid scales. Moist convection is generally triggered by
warming and moistening at low levels, which create convective instability, and
is manifested by narrow updrafts and downdrafts that can rarely be resolved
explicitly \parencite{mcfarlane2011}. Convection transports heat and moisture
vertically, removing the instability and generating storms where the condensed
water falls out as precipitation; more broadly, it is a key component of the
global atmospheric circulation in spite of its small spatial scale
\parencite{christensen2022}.

It should be noted that parametrisation also encompasses estimation techniques
for processes that are too complex to model exactly for reasons unrelated to
spatial and temporal resolution \parencite{mcfarlane2011}. A good example of
such a process is \emph{radiative transfer}. Air and its constituent gases, as
well as clouds, absorb, emit, reflect and/or scatter solar and terrestrial
radiation. This leads to differential heating and cooling that drives the
atmospheric circulation. While the theory of radiative transfer is understood
well enough to allow precise calculations in principle, the prohibitive
computational cost of such calculations necessitates a parametrisation based on
simplifying assumptions (the details of which are beyond the scope of this
review) \parencite{christensen2022}.


\subsection{Traditional solutions to the problem and their limitations}%
\label{sec:traditional}
With the examples of cloud microphysics, moist convection and radiative
transfer in mind, I now broadly review the traditional approaches used to
construct parametrisation schemes in practice. In particular, this section will
identify the key assumptions upon which many traditional schemes are founded,
and the circumstances under which the assumptions may be violated. This will
motivate research into novel approaches.

% conceptual model (examples)
A parametrisation scheme is traditionally constructed by formulating a
simplified, easily solvable and deterministic conceptual model of the physical
process in question. The solution of this model is then used to estimate the
effect of the process on the coarse-scale state of the parent model, known as
the \emph{unresolved tendency} \parencite{mcfarlane2011}. For example, the
earliest convective parametrisation was developed by \textcite{manabe1965} and
simply assumed that the net effect of convection is to relax the vertical
structure of the atmosphere towards a neutrally stable state whenever it
becomes convectively unstable.
% motivate data-driven parametrisation here?
One obvious deficiency of simple conceptual models is that they cannot possibly
capture the full range of variability in the processes they simulate. One major
branch of modern parametrisation research therefore studies \emph{data-driven}
schemes that instead use observational or high-resolution simulated data to fit
empirical models for the unresolved processes, naturally capturing a wider
range of variability \parencite{christensen2022}. Data-driven parametrisation
will be discussed in much further detail in the next section.

% grid-box mean and scale separation
In general, a traditional parametrisation scheme aims to capture the net
unresolved tendency due to all occurrences of the unresolved process (e.g., all
convective updrafts and downdrafts) within each grid cell of the parent model.
A deterministic prediction of this type is valid when each grid cell contains
many independent realisations whose varying contributions may be expected to
yield a reliable average tendency. This requires a \emph{scale separation}
between the unresolved process and the resolution of the parent model. Scale
separation breaks down when model development and increases in available
computing resources allow simulations at resolutions approaching the previously
unresolved scales. In this case, knowledge of the coarse-scale state cannot be
expected to uniquely determine the unresolved tendency because the process is
only realised a few times in each grid cell. The resulting (seemingly) random
nature of the true unresolved tendency motivates stochastic treatments
\parencite{mcfarlane2011,christensen2022,berner2017}. These will be discussed
in the next section.

% closure, quasi-equilibrium -> memory
The conceptual models core to traditional parametrisations usually contain free
parameters that are determined from the coarse-scale model state using
additional assumptions called \emph{closures}. Closures often postulate a state
of quasi-equilibrium between the unresolved processes and their large-scale
environment, such as a balance between the accumulation of convective available
potential energy (CAPE) and its removal by convection, or between the
horizontal convergence of moisture at low levels and its convective transport
to higher levels \parencite{mcfarlane2011,christensen2022,palmer2019}. However,
there is no guarantee that such an equilibrium exists; in fact, it has been
demonstrated that the CAPE balance is violated by fluctuations on sub-diurnal
time scales \parencite{donner2003} and by midlatitude continental convection
\parencite{zhang2002}. Newer parametrisation schemes have allowed departures
from equilibrium by representing the unresolved processes in a prognostic
rather than diagnostic manner (i.e., allowing the processes to have their own
self-governing time dependence rather than calculating them from the
large-scale state at each time step independently of their values at the
previous step) \parencite{rio2019,berner2017}. This creates \emph{memory} or
\emph{latency} in the parametrised tendencies, meaning that the tendencies have
some nonzero response time when subjected to sudden changes in the large-scale
state. Memory will be discussed further in the next section.

% ISSUES
Parametrisation schemes commonly suffer from several other issues that I will
briefly address here.
% division of processes -> unification
Firstly, while the division of the general atmospheric dynamics into a set of
separately parametrised processes (microphysics, convection, etc.) is
physically motivated, it remains somewhat arbitrary because these processes,
strictly speaking, form a continuum without well-defined boundaries
\parencite{christensen2022,mcfarlane2011}. It is a goal of contemporary
research to unify the parametrisations as much as possible.
% universality
Secondly, given the importance of future climate projections, it is natural to
ask whether the parametrisation schemes that have been developed and tuned on
today's climate remain valid as the climate changes over decade- to
century-long simulations. This is a matter of particular concern for
data-driven parametrisations, since there is little reason to trust empirical
models once they are extrapolated beyond the range of the data originally used
to fit them \parencite{christensen2022}.
% conservation laws
Finally, unless very special care is taken, parametrisation schemes can cause
the parent model to violate known physical conservation laws (e.g., mass,
energy and momentum) \parencite{christensen2022}. Efforts to resolve this issue
are ongoing.


\section{Novel approaches to the parametrisation problem}%
\label{sec:novel}
% ongoing issues, processes will remain unresolved
Since the 1990s, the limitations identified in \cref{sec:traditional} have
prompted many to reconsider the principles upon which parametrisation schemes
are founded. The main advance has been the development of stochastic
parametrisations that incorporate random noise in the predicted tendencies.
More recently, others have approached the problem from an entirely new
direction, developing data-driven parametrisations that learn to predict the
tendencies empirically. This section will introduce the stochastic and
data-driven approaches in general terms while omitting the technical details of
individual implementations, the aim being to contextualise and motivate the
study of these approaches in more idealised frameworks in \cref{chap:lorenz96}.
Other methods (such as superparametrisation) exist but are beyond the scope of
this review.


\subsection{Stochastic parametrisation and memory} \label{sec:stochastic}
The potential value of stochasticity for climate modelling was first
established by \textcite{hasselmann1976}, whose seminal paper sought to explain
the characteristics of long-term climate variability. Knowing that climate
depends on interactions between all components of the Earth system
(atmosphere, ocean, cryosphere, biosphere, etc.), \citeauthor{hasselmann1976}
argued that the effect of the more rapidly-evolving atmosphere on the other,
more slowly-varying components is that of a stochastic forcing. Owing to
their long response time, the slowly-varying components effectively integrate
this stochastic forcing, allowing long-term climate variability to be
characterised as a type of random walk process akin to the Brownian motion
of a massive particle in a fluid.

Further motivation for stochastic parametrisation in particular stems from the
need to reliably estimate uncertainties in weather forecasts and climate
projections. Weather forecasting centres typically propagate initial condition
uncertainties (due to imperfect observations) through to the final forecast in
a Monte Carlo fashion, by initialising an ensemble of model runs with perturbed
initial conditions and measuring the spread of the resulting forecasts. It has
been observed that deterministic models produce systematically under-dispersed
ensembles that fail to span the range of actual weather outcomes, indicating
that these models are failing to capture additional sources of variability
\parencite{palmer2005,berner2017,palmer2019}. Knowing that poor scale
separation and departures from quasi-equilibrium should preclude deterministic
relationships between unresolved tendencies and the large-scale state (see
\cref{sec:traditional}), it should seem highly likely that deterministic
parametrisation contributes to this deficiency.

% should this go at the beginning of the subsection so we get the point quicker?
The principle of stochastic parametrisation is, therefore, that unresolved
tendencies should be randomly sampled from an appropriate distribution at each
point in space and time, not simply set to the mean of the distribution
\parencite{franzke2015}. This choice has now been theoretically justified using
statistical mechanical arguments; most notably,
\textcite{wouters2012,wouters2013} showed that, assuming a weak coupling to the
resolved variables, the effect of unresolved dynamics should be parametrised by
a combination of deterministic and stochastic terms, as well as a
\emph{non-Markovian} memory term depending on the past states of the resolved
variables.

The simplest and earliest approach to stochastic parametrisation is the method
of \emph{stochastically perturbed parametrisation tendencies} (SPPT), which
takes an existing deterministic parametrisation and randomly scales its output
with a multiplicative noise field \parencite{palmer2019,christensen2020}.
Re-using the notation of \cref{sec:math}, an SPPT model for the resolved
variables $\vec{x}$ takes the form
\begin{equation*}
    \diff{\vec{x}}{t}
        = \vec{D}(\vec{x},t)
        + [I + \operatorname{diag}(\vec{e})] \vec{P}(\vec{x}, t),
\end{equation*}
where $\vec{P}$ is the existing deterministic parametrisation, $\vec{e}$ is a
mean-zero random vector with the same length as $\vec{x}$ and $I$ is the
identity matrix. The choice of multiplicative (and thus inherently
state-dependent) rather than additive noise is intuitively motivated by the
expectation that variability in the effect of unresolved processes should be
greatest when those processes are most active \parencite{franzke2015}.
\textcite{christensen2020} performed a comparison of high-resolution
simulations to parametrised single-column model output which justified the use
of multiplicative perturbations.

In GCMs, where the variables being modelled have spatial dependence, the
multiplicative perturbation takes the form of a random \emph{field}
$e(x,y,t)$. It has been argued that this random field should be spatially and
temporally correlated (in contrast to uncorrelated ``white'' noise) in order to
emulate the organisation of unresolved processes on larger scales and their
persistence in time \parencite{christensen2022,franzke2015}. In particular,
perturbations that explicitly depend on their own past values constitute a type
of memory, albeit distinct from the memory term advocated by
\textcite{wouters2012,wouters2013}, which would instead couple the
perturbations to the past values of the large-scale variables.

Stochastic parametrisations have several known advantages over their
deterministic counterparts, and are now implemented in operational weather
forecast models. They have been found to remedy the aforementioned issues of
ensemble underdispersion and prediction unreliability
\parencite{palmer2005,berner2017}, and (as of 2009) even make the skill of
five-day forecasts comparable to that of deterministically parametrised two-day
forecasts \parencite{palmer2019}. Despite the zero-mean nature of the noise, it
has been shown that stochastic parametrisations can reduce systematic model
biases (``noise-induced drift''; \textcite{palmer2005}) and stabilise the
simulation of regime-based behaviour (such as the El Ni\~{n}o-Southern
Oscillation) in the climate system \parencite{berner2017}.

Reviewing the field, \textcite{palmer2019} identifies outstanding issues for
further research. The main concern is the lack of rigour in most stochastic
schemes; SPPT, for example, modifies existing deterministic schemes \emph{ad
hoc} rather than incorporating stochasticity \emph{ab initio} in the
development process. More objective approaches are yet to gain widespread
acceptance. This motivates both data-driven methods and further testing
using simpler dynamical systems where objectivity is more feasible.


\subsection{Data-driven parametrisation and machine learning}%
\label{sec:data_driven}
The fitting of predictive statistical models to data is, of course, ubiquitous
in the sciences, but attempts to use such models as parametrisation schemes and
couple them into fully fledged GCMs are a relatively new phenomenon---certainly
more so than stochastic parametrisations. The prerequisite for all data-driven
parametrisations is obviously training data: mathematically, an approximate
solution of \cref{eqn:truth} of sufficient accuracy and resolution to be
considered ``truth'' for the application at hand. Training data are typically
derived from high-resolution simulations, such as regional weather model runs
or large eddy simulations (LES). The other ingredient is the imperfect
low-resolution model that will later be augmented by parametrisation.

To generate an appreciation of how these data might be used to construct a
parametrisation in practice, I roughly follow the argument and notation of
\textcite{brajard2021}. Define a map $\mathcal{M}$ that takes each state
$\vec{z}(t)$ in the training dataset to the state $\vec{z}(t + \delta t)$ at
the next time step. Similarly, denote by $\mathcal{M}^\mathrm{r}$ the
low-resolution model (the superscript r meaning ``reduced''), which maps a
low-resolution state $\vec{x}(t)$ to a prediction for $\vec{x}(t + \Delta t)$,
where $\Delta t$ is not necessarily equal to $\delta t$ (usually larger). The
spaces of high- and low-resolution states, having different dimensions, may be
linked by an operator $\langle \cdot \rangle$ that projects high-resolution
states onto the low-resolution state space. Practically, the projection
operation can simply be a coarse-graining of $\vec{z}$ by averaging values at
adjacent grid points to match the resolution of $\vec{x}$. Now, for each state
$\vec{z}$ in the training dataset, one may compute the difference
\begin{equation*}
    \vec{\epsilon}(\vec{z}) =
        \frac{
            \langle \mathcal{M}(\vec{z}) \rangle - \langle \vec{z} \rangle
        }{\delta t}
        - \frac{
            \mathcal{M}^\mathrm{r}(\langle \vec{z} \rangle)
            - \langle \vec{z} \rangle
        }{\Delta t}
\end{equation*}
between the coarse-grained true tendency (on the left) and the tendency
predicted by the coarse model when it sees the same state (on the right). This
quantity is known as the \emph{subgrid} or \emph{unresolved} tendency.
Knowledge of $\langle \vec{z} \rangle$ alone does not uniquely determine
$\vec{\epsilon}$ because $\langle \mathcal{M}(\vec{z}) \rangle$ depends on the
original $\vec{z}$. However, if one can fit a statistical model $\vec{P}$ to
the dataset of ($\langle \vec{z} \rangle$, $\vec{\epsilon}$), then
$-\vec{P}(\vec{x}(t)) \Delta t$ will be able to serve as an estimate of the
error incurred by the coarse model in predicting $\vec{x}(t + \Delta t)$ given
$\vec{x}(t)$. This motivates the construction of a parametrised model
$\mathcal{M}^\mathrm{p}$, defined by
\begin{equation*}
    \mathcal{M}^\mathrm{p}(\vec{x}) =
        \mathcal{M}^\mathrm{r}(\vec{x}) + \vec{P}(\vec{x}) \Delta t
\end{equation*}
that simply subtracts the estimated error at each time step of the coarse
model. In theory, the solution $\vec{x}(t)$ obtained by iteration of
$\mathcal{M}^\mathrm{p}$ will be more accurate than that obtained by
iteration of the unparametrised $\mathcal{M}^\mathrm{r}$.

The current method of choice for constructing $\vec{P}$ in weather and climate
modelling contexts is machine learning (ML). I will briefly introduce the key
concepts, strengths and weaknesses of ML, following \textcite{beucler2022}. ML
encompasses a broad class of algorithms that programmatically and autonomously
develop and apply rules for performing tasks, such as image classification,
removing the need for a human to explicitly program the rules. They do this
using supplied \emph{training data} that exemplify the task to be performed.
When it comes to data-driven parametrisation, the task is function
approximation or regression, and the relevant algorithms usually require a
training dataset of example inputs (i.e., coarse-scale variables)
\emph{labelled} with the desired outputs (i.e., unresolved tendencies). One
commonly-used class of algorithms called \emph{neural networks} consist of
layers of intercommunicating calculation nodes called neurons and learn to
approximate functions by optimising sets of weights associated with the
neurons. Another type of algorithm is the \emph{random forest}, which
approximates functions by passing the inputs through a series of nested
\texttt{if/else} decision trees.

The distinguishing advantage offered by ML data-driven parametrisations is
their ability to systematically capture complex relationships without relying
on artificially simplified physical models derived by humans
\parencite{irrgang2021,beucler2022}. In addition to constructing new
parametrisations from scratch, ML has been used to emulate the action of
existing parametrisation schemes in GCMs \parencite[e.g.,][]{gentine2018}.
These ML emulators have the potential to reduce the computational burden that
parametrisation usually imposes, leaving more resources for other tasks such as
calling the parametrisation scheme more frequently or increasing the model
resolution \parencite{beucler2022}. Furthermore, the ability to emulate
multiple existing schemes at once is a step towards unifying the
parametrisation of unresolved processes (a long-standing issue; see
\cref{sec:traditional}).

ML parametrisations are not without their issues; they are known to be prone to
overfitting and often cause their host models to become numerically unstable.
They are also, by their very nature, far more difficult to interpret than
traditional schemes. It is therefore very difficult to determine \emph{a
priori} whether they will continue to be valid in climate conditions
outside the bounds of their training datasets
\parencite{irrgang2021,beucler2022}. ``Interpretable'' or ``explainable'' ML is
an active area of research that aims to address these issues. Another active
research topic is ``physics-guided ML'', the aim of which is to augment ML with
known physical constraints and conservation laws that would otherwise be
violated \parencite[e.g.,][]{yuval2021}.

\section{Summary}
This chapter has highlighted the issues associated with traditional
parametrisation schemes (including the limitations of conceptual models, lack
of scale separation and quasi-equilibrium assumptions) and reviewed the modern techniques
(including stochasticity, memory and data-driven/ML modelling) that have been
developed to address them. However, even the modern techniques give rise to
questions that lack definitive answers---should stochastic perturbations
be additive or multiplicative, spatially/temporally correlated or white?
How can empirical data-driven schemes be interpreted and explained physically?
I now turn to simpler dynamical system analogues of the climate system that
could be used to address these questions.


\ifSubfilesClassLoaded{%
    \emergencystretch=5em
    \printbibliography{}
}{}

\end{document}
