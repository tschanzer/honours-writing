% imports
\documentclass[titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={16cm, 23cm}, top=3.5cm]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage{
    amsmath,
    amssymb,
    amsthm,
    fancyhdr,
    siunitx,
    bm,
    lipsum,
    standalone,
    tikz,
    booktabs,
    enumitem,
    array,
}
\usepackage[colorlinks=true, allcolors=linkcolor]{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage[
    backend=biber,
    bibstyle=ext-authoryear,
    citestyle=ext-authoryear-comp,
    sorting=nyt,
    uniquename=false,
    maxbibnames=99,
    giveninits=true,
]{biblatex}

% bibliographic options
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{pages}{#1}
\renewcommand*{\volnumdelim}{}
\renewbibmacro{in:}{}
\addbibresource{references.bib}
\DeclareFieldFormat{titlecase:title}{\MakeSentenceCase*{#1}}

% ref options
\crefname{section}{\S}{\S\S}
\crefname{subsection}{\S}{\S\S}
\crefname{equation}{}{}
\crefname{figure}{Figure}{Figures}
\newcommand\crefrangeconjunction{--}
\def\equationautorefname~#1\null{(#1)\null}
\numberwithin{equation}{section}
\definecolor{linkcolor}{RGB}{51, 54, 142}

\makeatletter
\patchcmd{\math@cr@@@align}{\cr}{\global\let\df@label\@empty\cr}{}{}
\makeatother

% page style options
\pagestyle{fancy}
\fancyhead{}
\rhead{Title}
\renewcommand{\headrulewidth}{0.5pt}
\setlength{\headheight}{15pt}
\setlength\parindent{0pt}
\setlength\parskip{6pt}

% math macros
\renewcommand{\d}[1]{\mathrm{d}#1}
\newcommand{\diff}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
\newcommand{\ddiff}[2]{\frac{\mathrm{d}^2 #1}{\mathrm{d} {#2}^2}}
\newcommand{\pdiff}[2]{\frac{\partial #1}{\partial #2}}
\renewcommand\vec{\bm}
\newcommand{\uvec}[1]{\vec{\hat{#1}}}
\newcommand{\grad}{\vec{\nabla}}
\newcommand{\prandtl}{\ensuremath{\mathrm{Pr}}}
\newcommand{\rayleigh}{\ensuremath{\mathrm{Ra}}}
\newcommand{\nusselt}{\ensuremath{\mathrm{Nu}}}
\renewcommand{\bar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

% text macros
\newcommand{\rb}{Rayleigh-B\'{e}nard}

\begin{document}
\begin{titlepage}
\vfill~

\begin{center}
    {\Huge \textbf{%
        Title
    }} \\
    \vspace{0.75cm}
    {\Large\textbf{Honours Literature Review}} \\
    \vspace{0.75cm}
    {\Large\textbf{Thomas D. Schanzer}} \\
    \vspace{6pt}
    {\large Supervisor: Prof. Steven Sherwood} \\
    \vspace{0.75cm}
    {\large%
        School of Physics

        Climate Change Research Centre and
        ARC Centre of Excellence for Climate Extremes

        University of New South Wales, Sydney, Australia
    }
\end{center}
\vfill
\begin{center}
{\large\textbf{Abstract}}

\begin{minipage}{13cm}
    Text
\end{minipage}
\end{center}
\vfill
\end{titlepage}

\newpage
\tableofcontents

\newpage
\pagestyle{fancy}
\thispagestyle{fancy}

\section{Introduction and background}
\subsection{The necessity of parametrisation}
% Broad introduction of weather/climate prediction and fluid equations
The primary task of numerical weather and climate models is to simulate the
dynamics of the atmosphere and ocean, which are governed by the Navier-Stokes
equations. The algorithm chosen to solve these partial differential equations
is known as a model's \emph{dynamical core} \parencite{mcfarlane2011}. Since
analytical solutions to the equations do not exist, the dynamical core
necessarily approximates the continuous equations with finite-dimensional,
exactly solvable alternatives using one of several possible discretisation
schemes (e.g., the finite difference, finite element, finite volume and
spectral methods) \parencite{christensen2022}. In practice, this usually
involves representing the prognostic variables (i.e., those that affect the
evolution of the flow) with sets of discrete samples in space and time, whose
resolution is constrained by the available computing resources. The unavoidable
consequence of discretisation is the loss of information about processes
occurring on spatial and temporal scales smaller than the corresponding
sampling intervals. These processes are said to be \emph{unresolved}.

% why we need parametrisation: nonlinearity
It is tempting to na\"{i}vely accept the loss of fine-scale information as a
necessary sacrifice and hope the dynamical core will still make accurate
predictions for the larger, resolved scales. Unfortunately, this too is
impossible due to the \emph{nonlinearity} of the governing equations. It is
instructive to consider the linear case as a counterexample.

Linear differential equations allow arbitrary superpositions of solutions,
meaning that any given solution may be partitioned into high- and low-frequency
components, both of which are also solutions. One thus has the freedom to solve
for the low-frequency components alone without compromise. This property may be
understood more formally using the Fourier transform, defined for a function
$f$ of space and time by
\[
    \tilde{f}(\omega, \vec{k})
        = \int \d{t}\,\d^3{x}\, e^{i(\vec{k} \cdot \vec{x} - \omega t)}
        f(t, \vec{x}),
\]
which reduces any linear differential equation to an algebraic equation
relating the frequency $\omega$ to the wave vector $\vec{k}$. Each wavenumber
component of the initial state propagates trivially according to its own
time dependence $e^{-i(\vec{k} \cdot \vec{x} - \omega(\vec{k}) t)}$,
\emph{independently of the other components}. The fine-scale dynamics may
be safely neglected because they have no influence on the coarse scales.

The consequence of the nonlinearity of the equations governing atmospheric and
oceanic flows is, therefore, a coupling of the resolved coarse scales to the
unresolved fine scales \parencite{mcfarlane2011}.
% justification via Reynolds averaging
This fact may be demonstrated more explicitly by applying so-called
\emph{Reynolds averaging} to the equations \parencite{christensen2022}.
Reynolds averaging decomposes each field $q$ into the sum of a coarse-grained
(in space or time) or statistical-ensemble-averaged field $\bar{q}$ and a
perturbation $q'$. Note that $\bar{q'} = 0$ by definition. The coarse-graining
operation is assumed to be linear, commute with differentiation and satisfy
$\bar{\bar{p} q} = \bar{p} \bar{q}$ for any two fields $p$ and $q$
\parencite{monin2007}. Following the example given by
\textcite{christensen2022}, consider the incompressible Navier-Stokes
equations, which have the general form
\begin{align*}
    \pdiff{u_i}{t} + u_j \pdiff{u_i}{x_j} &= \sum f_i, \\
    \pdiff{u_i}{x_i} &= 0
\end{align*}
where $u_i$ are the components of the velocity, $x_i$ are the
coordinates and $f_i$ are various forces per unit mass. Summation over repeated
indices is implied. Applying the decomposition and coarse-graining both sides
of the equations yields
\begin{alignat*}{2}
    && \pdiff{}{x_i} \left( \bar{u_i} + u_i' \right) &= 0 \\
    \Rightarrow & \qquad &
        \pdiff{\bar{u_i}}{x_i}
        + \underbrace{\pdiff{\bar{u_i'}}{x_i}}_{=0} &= 0 \\
    \Rightarrow & \qquad &
        \pdiff{\bar{u_i}}{x_i} = \pdiff{u_i'}{x_i} &= 0
\end{alignat*}
and
\begin{alignat*}{2}
    && \sum \bar{f_i} &=
        \bar{\pdiff{}{t} \left( \bar{u_i} + u_i' \right)}
        + \bar{\left( \bar{u_j} + u_j' \right)
        \pdiff{}{x_j} \left( \bar{u_i} + u_i' \right)} \\
    &&&=
        \pdiff{\bar{u_i}}{t}
        + \underbrace{\pdiff{\bar{u_i'}}{t}}_{=0}
        + \bar{u_j} \pdiff{\bar{u_i}}{x_j}
        + \bar{u_j} \underbrace{\pdiff{\bar{u_i'}}{x_j}}_{=0}
        + \underbrace{\bar{u_j'}}_{=0} \pdiff{\bar{u_i}}{x_j}
        + \bar{u_j' \pdiff{u_i'}{x_j}} \\
    &&&=
        \pdiff{\bar{u_i}}{t} + \bar{u_j} \pdiff{\bar{u_i}}{x_j}
        + \pdiff{\bar{u_i' u_j'}}{x_j}
        - \bar{u_i' \underbrace{\pdiff{u_j'}{x_j}}_{=0}} \\
    \Leftrightarrow & \qquad &
        \pdiff{\bar{u_i}}{t} + \bar{u_j} \pdiff{\bar{u_i}}{x_j}
        &= \sum \bar{f_i} - \pdiff{\bar{u_i' u_j'}}{x_j}.
\end{alignat*}
The physical meaning of this last equation is that the evolution of the
coarse-grained velocity field depends not only on itself and the coarse-grained
forces, but also on the perturbations via $\bar{u_i' u_j'}$, which does not
necessarily vanish. This dependence is a consequence of nonlinearity.

The theoretical discussion in this section establishes that physical processes
occurring at one place in the spectrum of temporal and spatial scales are
coupled to all the other processes in the spectrum \parencite{franzke2015}. In
particular, to ignore the effect of processes not explicitly resolved in
numerical models would introduce unacceptable systematic biases, or errors, in
the model forecasts. Numerical weather and climate models therefore require
parametrisation schemes to estimate the effects of unresolved processes as
functions of the available large-scale information (and the parameters of these
functions must be chosen appropriately---hence the name ``parametrisation'').


\subsection{Mathematical formulation of the parametrisation problem}


\subsection{Physical processes requiring parametrisation}
% spectrum of scales and implications of cross-interaction
% Examples of processes requiring parametrisation
Following the broad theoretical argument in the previous section, I now turn to
concrete examples of processes that are often represented by parametrisations,
restricting the discussion to atmospheric processes for brevity. This section
will give a basic introduction to each process, its effect on the larger-scale
behaviour of the climate system and the role of its corresponding
parametrisation scheme. The purpose of these examples is to provide real-world
context and motivation for current parametrisation research in more idealised
settings, which will form one of the main topics of this review.

\emph{Cloud microphysics} parametrisations model the composition of clouds in
terms of the amount of water in the solid (e.g., hail), liquid (cloud droplets
and rain) and gaseous phases, and the rate of transitions between these phases.
Accurate representation of cloud formation and evolution is crucial for several
reasons. First, the interaction of clouds with solar and terrestrial radiation
has a major influence on the overall energy balance of the atmosphere
\parencite{mcfarlane2011}. Second, cloud water phase transitions lead to the
formation of precipitation, and are a source and sink of latent heat that
drives convection \parencite{mcfarlane2011}. Furthermore, the statistics of
cloud formation are linked to global temperatures in a feedback loop;
uncertainty in the sign and magnitude of this feedback effect is a major
contributor to uncertainty in the sensitivity of global temperatures to
increases in atmospheric CO$_2$ concentration \parencite{andrews2012,
christensen2022,stevens2013}. Microphysical processes occur on the spatial
scales of single water droplets and ice particles---among the smallest scales
in the atmosphere. All atmospheric models must parametrise the amount of cloud
water in each phase, the size distribution and concentration of liquid and ice
particles, and the resulting rate and type (rain, hail, snow, etc.) of
precipitation \parencite{christensen2022}.

\emph{Moist convection} encompasses vertical motions in the atmosphere that are
accompanied (and driven) by phase changes of water, and must be parametrised
when it occurs on sub-grid scales. Moist convection is generally triggered by
warming and moistening at low levels, which create convective instability, and
is manifested by narrow updrafts and downdrafts that can rarely be resolved
explicitly \parencite{mcfarlane2011}. Convection transports heat and moisture
vertically, removing the instability and generating storms where the condensed
water falls out as precipitation; more broadly, it is a key component of the
global atmospheric circulation in spite of its small spatial scale
\parencite{christensen2022}.

It should be noted that parametrisation also encompasses estimation techniques
for processes that are too complex to model exactly for reasons unrelated to
spatial and temporal resolution \parencite{mcfarlane2011}. A good example of
such a process is \emph{radiative transfer}. Air and its constituent gases, as
well as clouds, absorb, emit, reflect and/or scatter solar and terrestrial
radiation. This leads to differential heating and cooling that drives the
atmospheric circulation. While the theory of radiative transfer is understood
well enough to allow precise calculations in principle, the prohibitive
computational cost of such calculations necessitates a parametrisation based on
simplifying assumptions (the details of which are beyond the scope of this
review) \parencite{christensen2022}.


\subsection{Traditional solutions to the problem and their limitations}
With the examples of cloud microphysics, moist convection and radiative
transfer in mind, I now broadly review the traditional approaches used to
construct parametrisation schemes in practice. In particular, this section will
identify the key assumptions upon which many traditional schemes are founded,
and the circumstances under which the assumptions may be violated. This will
motivate research into novel approaches.

% conceptual model (examples)
A parametrisation scheme is traditionally constructed by formulating a
simplfied, easily solvable and deterministic conceptual model of the physical
process in question. The solution of this model is then used to estimate the
effect of the process on the coarse-scale state of the parent model, known as
the \emph{unresolved tendency} \parencite{mcfarlane2011}. For example, the earliest convective
parametrisation was developed by \textcite{manabe1965} and simply assumed that
the net effect of convection is to relax the vertical structure of the
atmosphere towards a neutrally stable state whenever it becomes convectively
unstable.
% motivate data-driven parametrisation here?
One obvious deficiency of simple conceptual models is that they cannot possibly
capture the full range of variability in the processes they simulate. One major
branch of modern parametrisation research therefore studies \emph{data-driven}
schemes that instead use observational or high-resolution simulated data to fit
empirical models for the unresolved processes, naturally capturing a wider
range of variability \parencite{christensen2022}. Data-driven parametrisation
will be discussed in much further detail in the next section.

% grid-box mean and scale separation
In general, a traditional parametrisation scheme aims to capture the net
unresolved tendency due to all occurrences of the unresolved process (e.g., all
convective updrafts and downdrafts) within each grid cell of the parent model.
A deterministic prediction of this type is valid when each grid cell contains
many independent realisations whose varying contributions may be expected to
yield a reliable average tendency. This requires a \emph{scale separation}
between the unresolved process and the resolution of the parent model. Scale
separation breaks down when model development and increases in available
computing resources allow simulations at resolutions approaching the previously
unresolved scales. In this case, knowledge of the coarse-scale state cannot be
expected to uniquely determine the unresolved tendency because the process is
only realised a few times in each grid cell. The resulting (seemingly) random
nature of the true unresolved tendency motivates stochastic treatments
\parencite{mcfarlane2011,christensen2022,berner2017}. These will be discussed
in the next section.

% closure, quasi-equilibrium -> memory
The conceptual models core to traditional parametrisations usually contain free
parameters that are determined from the coarse-scale model state using
additional assumptions called \emph{closures}. Closures often postulate a state
of quasi-equilibrium between the unresolved processes and their large-scale
environment, such as a balance between the accumulation of convective available
potential energy (CAPE) and its removal by convection, or between the
horizontal convergence of moisture at low levels and its convective transport
to higher levels \parencite{mcfarlane2011,christensen2022,palmer2019}. However,
there is no guarantee that such an equilibrium exists; in fact, it has been
demonstrated that the CAPE balance is violated by fluctuations on sub-diurnal
time scales \parencite{donner2003} and by midlatitude continental convection
\parencite{zhang2002}. Newer parametrisation schemes have allowed departures
from equilibrium by representing the unresolved processes in a prognostic
rather than diagnostic manner (i.e., allowing the processes to have their own
self-governing time dependence rather than calculating them from the
large-scale state at each time step independently of their values at the
previous step) \parencite{rio2019,berner2017}. This creates \emph{memory} or
\emph{latency} in the parametrised tendencies, meaning that the tendencies have
some nonzero response time when subjected to sudden changes in the large-scale
state. Memory will be discussed further in the next section.

% ISSUES
Parametrisation schemes commonly suffer from several other issues that I will
briefly address here.
% division of processes -> unification
Firstly, while the division of the general atmospheric dynamics into a set of
separately parametrised processes (microphysics, convection, etc.) is
physically motivated, it remains somewhat arbitrary because these processes,
strictly speaking, form a continuum without well-defined boundaries
\parencite{christensen2022,mcfarlane2011}. It is a goal of contemporary
research to unify the parametrisations as much as possible.
% universality
Secondly, given the importance of future climate projections, it is natural to
ask whether the parametrisation schemes that have been developed and tuned on
today's climate remain valid as the climate changes over decade- to
century-long simulations. This is a matter of particular concern for
data-driven parametrisations, since there is little reason to trust empirical
models once they are extrapolated beyond the range of the data originally used
to fit them \parencite{christensen2022}.
% conservation laws
Finally, unless very special care is taken, parametrisation schemes can cause
the parent model to violate known physical conservation laws (e.g., mass,
energy and momentum) \parencite{christensen2022}. Efforts to resolve this issue
are ongoing.


\newpage
\section{Novel approaches to the parametrisation problem}
% ongoing issues, processes will remain unresolved

\section{Simple dynamical systems as testbeds for novel approaches}

\newpage
\section{Dynamical system case study: \rb{} convection}
\subsection{Problem statement}
\rb{} convection is the motion of a fluid confined between two
horizontal isothermal plates, the temperature of the bottom plate being
higher than that of the top plate. The governing equations for the flow
follow from the Navier-Stokes equations of mass, energy and momentum
conservation. The reader is referred to \textcite{chandrasekhar1961} for
a detailed derivation; I summarise the assumptions and approximations
involved below.

The density, $\rho$, of the fluid is related to its temperature $T$ by
the linear equation of state
\[
    \rho = \rho_0 [1 - \alpha(T - T_0)],
\]
where $\alpha$ is the (constant) volume coefficient of thermal expansion and
$\rho_0$ and $T_0$ are the base-state density and temperature such that $\rho =
\rho_0$ when $T = T_0$. The key assumption is that density variations are small
($\alpha (T - T_0) \ll 1$), which allows the governing equations to be
simplified under the \emph{Boussinesq approximation}. The Boussinesq
approximation involves first writing the pressure, $p$, of the fluid as
\[
    p = p_0 - \rho_0 gz + p',
\]
where $p_0$ is an arbitrary constant, $g$ is the acceleration due to gravity
and $z$ is the vertical coordinate. $p'$ is the (time-varying) deviation from
a hydrostatically balanced background profile $p_0 - \rho_0 gz$
in which the upward pressure gradient force per unit volume $\rho_0 g$ cancels
the downward weight force per unit volume $-\rho_0 g$. Since
$\alpha (T - T_0) \ll 1$, density variations are neglected everywhere except
in their contribution to the weight force, leading to a net buoyant
(background pressure gradient plus weight) force per unit mass
\[
    \frac{\rho_0 - \rho}{\rho_0} g = \alpha (T - T_0) g.
\]

With these assumptions in mind, I adopt the governing equations as they
are derived by \textcite{chandrasekhar1961}:
\begin{alignat}{2}
    \label{eqn:dim_momentum}
    \pdiff{\vec{u}}{t} + \vec{u} \cdot \grad \vec{u}
        &= -\frac{1}{\rho_0} \grad p' + \alpha (T - T_0) g \uvec{z}
        + \nu \nabla^2 \vec{u}
    &\quad& \text{(momentum conservation),} \\
    \label{eqn:dim_energy}
    \pdiff{T}{t} + \vec{u} \cdot \grad T
        &= \kappa \nabla^2 T
    && \text{(energy conservation), and} \\
    \label{eqn:dim_incompressible}
    \grad \cdot \vec{u} &= 0
    && \text{(incompressibility).}
\end{alignat}
$\vec{u}$ is the fluid velocity, $t$ is time, $\uvec{z}$ is the upward
unit vector, $\nu$ is the (constant) kinematic viscosity and $\kappa$ is
the thermal diffusivity (also constant). Notice that the aforementioned
buoyancy term $\alpha (T - T_0) g$ appears on the right-hand side of
\cref{eqn:dim_momentum}.

The parametrisation test-bed developed in this work solves the governing
equations in a two-dimensional domain $(x,z) \in [0, d] \times [0, L]$, subject
to no-slip, isothermal boundary conditions on the top and bottom plates,
\begin{alignat}{3}
    \label{eqn:dim_bc_bot}
    \vec{u} &= \vec{0}, &\quad T &= T_0 + \frac{\delta T}{2}
    &\qquad& \text{at } z = 0 \text{ and} \\
    \label{eqn:dim_bc_top}
    \vec{u} &= \vec{0}, &\quad T &= T_0 - \frac{\delta T}{2}
    &\qquad& \text{at } z = d,
\end{alignat}
and periodic boundary conditions in the horizontal,
\begin{alignat}{2}
    \label{eqn:dim_bc_sides}
    \vec{u}(x=0) &= \vec{u}(x=L) &\quad \text{and} \quad T(x=0) &= T(x=L).
\end{alignat}
$\delta T$ is the constant temperature difference between the plates.

\subsection{Nondimensionalisation and scale analysis}
It is helpful to nondimensionalise the governing equations
\crefrange{eqn:dim_momentum}{eqn:dim_bc_sides}; this is not only useful
for numerical work but also gives insight into the different flow
regimes that are possible. A range of nondimensionalisations are used in
fluid dynamics literature; I adopt a common one (see, e.g.,
\textcite{grotzbach1983,ouertatani2008,stevens2010}) which is suitable
for the turbulent convective regime.

For low-viscosity, turbulent flow, a suitable time scale is the
\emph{free-fall time} $t_0$, which is the time for a fluid element with
constant temperature $T = T_0 - \delta T$ to fall from the top plate to
the bottom plate under the influence of buoyancy ($-g \alpha \delta T$)
alone. It is simple to show that
\[
    t_0 \sim \left( \frac{d}{g \alpha \delta T} \right)^{1/2},
\]
ignoring a factor of $\sqrt{2}$. The obvious length and temperature
scales are the plate separation $d$ and temperature difference $\delta T$,
respectively.

Making the substitutions $p'/\rho_0 \to \pi$ and $T - T_0 \to \theta$
in \crefrange{eqn:dim_momentum}{eqn:dim_bc_sides} and expressing all
variables in units of $t_0$, $d$ and $\delta T$ leads to the dimensionless
equations
\begin{align}
    \label{eqn:momentum}
    \pdiff{\vec{u}}{t} + \vec{u} \cdot \grad \vec{u}
        &= -\grad \pi + \left( \frac{\prandtl}{\rayleigh}\right)^{1/2}
        \nabla^2 \vec{u} + \theta \uvec{z}, \\
    \label{eqn:energy}
    \pdiff{\theta}{t} + \vec{u} \cdot \grad \theta
        &= (\rayleigh\,\prandtl)^{-1/2} \, \nabla^2 \theta, \quad \text{and} \\
    \label{eqn:incompressible}
    \grad \cdot \vec{u} &= 0,
\end{align}
with boundary conditions
\begin{gather}
\begin{alignat}{3}
    \label{eqn:bc_bot}
    \vec{u} &= \vec{0}, &\quad \theta &= +\frac{1}{2}
    &\qquad& \text{at } z = 0, \\
    \label{eqn:bc_top}
    \vec{u} &= \vec{0}, &\quad \theta &= -\frac{1}{2}
    &\qquad& \text{at } z = 1,
\end{alignat} \\
\begin{alignat}{2}
    \label{eqn:bc_sides}
    \vec{u}(x=0) &= \vec{u}(x=\Gamma)
    &\quad \text{and} \quad \theta(x=0) &= \theta(x=\Gamma).
\end{alignat}
\end{gather}
There are three dimensionless parameters: the aspect ratio of the domain
\[
    \Gamma \equiv \frac{L}{d},
\]
the \emph{Prandtl number}
\[
    \prandtl \equiv \frac{\nu}{\kappa}
\]
which measures the relative importance of viscosity (momentum diffusivity)
and thermal diffusivity, and the \emph{Rayleigh number}
\[
    \rayleigh \equiv \frac{g \alpha d^3 \delta T}{\kappa \nu}.
\]
The Rayleigh number can be interpreted as the ratio of the time scale for
thermal transport by convection to the time scale for thermal transport by
conduction. It determines the importance of diffusion for the evolution of
$\vec{u}$ and $\theta$; inspection of \cref{eqn:momentum,eqn:energy} indicates
that low $\rayleigh$ implies strong diffusion and high $\rayleigh$ weak
diffusion. Detailed theoretical analysis of the governing equations (see, e.g.,
\textcite{chandrasekhar1961} and the seminal work by \textcite{rayleigh1916})
reveals that there exists a critical Rayleigh number (dependent on boundary
conditions but of order $\SI{e3}{}$), below which the equations have a stable
equilibrium with the fluid at rest and a linear conductive temperature profile.
Above the critical value, the equilibrium is unstable and small perturbations
lead to the formation of a regular series of steady, rotating convection cells.
If the Rayleigh number is increased much further (\textcite{le_quere1991} cites
$\rayleigh \approx \SI{2e8}{}$), the solution becomes unsteady and increasingly
turbulent. This work is concerned with the turbulent regime, since Rayleigh
numbers for atmospheric deep moist convection can be as large as $\SI{e22}{}$
\parencite{chilla2012}.

\subsection{Thermal properties}
The rate of heat transport across the fluid layer has physical significance for
natural realisations of thermally driven convection. It is widely analysed in
terms of the dimensionless \emph{Nusselt number}, whose accurate determination
is a common theme in the \rb{} convection literature; I therefore introduce
this quantity before proceeding. The Nusselt number measures the rate of
(vertical) heat transport across a horizontal plane at height $z$, normalised
by the purely conductive rate that would exist if the fluid were at rest
\parencite{verzicco1999}. Following \textcite{chilla2012}, I use the definition
(before nondimensionalisation)
\begin{equation}
    \label{eqn:dim_nusselt}
    \nusselt(z,t) \equiv \frac{
        \langle wT \rangle_{A,t}
        - \kappa \partial \langle T \rangle_{A,t} / \partial z
    }{
        \kappa \delta T / d
    }
\end{equation}
where $\langle \cdot \rangle_{A,t}$ denotes averaging over time and the
horizontal plane at height $z$, and $w = \vec{u} \cdot \uvec{z}$ is the
vertical velocity. The rate of heat transport in the numerator has two terms:
advection $\langle wT \rangle_{A,t}$ and conduction $-\kappa \partial \langle T
\rangle_{A,t} / \partial z$. The denominator $\kappa \delta T / d$ is the rate
of heat transport for a linear conductive temperature profile with the fluid at
rest. \textcite{scheel2013} give the corresponding nondimensionalised form,
\begin{equation}
    \label{eqn:nusselt}
    \nusselt(z,t) \equiv (\rayleigh\,\prandtl)^{1/2}
        \langle w \theta \rangle_{A,t}
        - \pdiff{\langle \theta \rangle_{A,t}}{z}.
\end{equation}

Another important quantity is the thickness $\delta_T$ of the \emph{thermal
boundary layer} at each plate where large temperature gradients exist.
\textcite{chilla2012} define $\delta_T$ as follows: if, on average, the fluid
temperature changes with height from $+\delta T/2$ at the lower plate to $0$
(the mean value in the well-mixed interior) over a distance $\delta_T$, then
\[
    \left. \pdiff{\langle T \rangle_{A,t}}{z} \right|_{z=0}
        \approx -\frac{\delta T}{2 \delta_T}.
\]
But if one considers the definition of the Nusselt number
\cref{eqn:dim_nusselt} at $z=0$, the advection term $\langle wT \rangle_{A,t}$
vanishes due to the $\vec{u} = \vec{0}$ boundary condition and
\[
    \nusselt(z=0) = -\frac{d}{\delta T}
        \left. \pdiff{\langle T \rangle_{A,t}}{z} \right|_{z=0}.
\]
Thus,
\begin{equation}
    \label{eqn:thermal_bl}
    \delta_T = \frac{d}{2\,\nusselt(z=0)}.
\end{equation}


\subsection{Resolution dependence of numerical solutions}
% s.sherwood: Might be important to further explain here that you will regard
% this as the ("warts and all") dynamical system you want to emulate at a
% coarser representation.  This avoids the issue of whether this hig-res is a
% perfect solution of the fluid equations or not (it doesn't have to be,
% although for robustness we want it to be relatively insensitive to small
% changes in dx).
In constructing a parametrisation test-bed, I will firstly seek
reasonably accurate, high-resolution ``truth'' solutions of
\crefrange{eqn:momentum}{eqn:bc_sides}. I will then deliberately reduce
the resolution, aiming to produce solutions that are sufficiently
``different'' that they might reasonably be ``improved'' by a parametrisation
scheme. In this section, I review relevant literature with the aim of
establishing how, exactly, under-resolved simulations of \rb{} convection
might ``differ'' from well-resolved ones, and thus what one would hope to
``improve''. Specifically, I will address the following questions:
\begin{itemize}
    \item What resolution is necessary for a converged solution?
    \item Which quantities are most sensitive to insufficient resolution?
\end{itemize}

\subsubsection{Theoretical resolution requirements for accurate simulations}%
\label{sec:res_requirements}

% resolution in general, Grotzbach condition.
\textcite{grotzbach1983} is recognised as the first to formulate resolution
requirements for accurate simulations of \rb{} convection
\parencite{chilla2012,scheel2013}. He identified separate constraints on the
mean (i.e., averaged in each spatial direction) grid spacing and the vertical
spacing near the plates; I first discuss the former.
\citeauthor{grotzbach1983} reasoned that a numerical model that neglects
subgrid-scale effects must have a geometric mean grid spacing $h = (\Delta x
\Delta y \Delta z)^{1/3}$ such that
\begin{equation}
    \label{eqn:grotzbach}
    h \leq \pi \eta = \pi \left(
        \frac{\nu^3}{\langle \epsilon \rangle}
    \right)^{1/4}
\end{equation}
where $\eta \equiv (\nu^3/\epsilon)^{1/4}$ is the \emph{Kolmogorov length}, the
universal smallest relevant length scale for general turbulent flow, and
$\langle \epsilon \rangle$ is the spatial and temporal average of the kinetic
energy dissipation rate defined by
\begin{equation}
    \label{eqn:kinetic_dissipation}
    \epsilon(\vec{x}, t) \equiv \frac{\nu}{2} \sum_{ij} \left(
        \pdiff{u_i}{x_j} + \pdiff{u_j}{x_i}
    \right)^2
\end{equation}
\parencite{chilla2012}. The condition \cref{eqn:grotzbach} can be
understood using the Nyquist-Shannon theorem, which states that a
sampling frequency $f \geq k/\pi$ is needed to unambiguously reconstruct
a signal with maximum wavenumber $k$; substituting $f = 1/h$, $k =
1/\eta$ leads to the claimed relation.

% resolution near plates, number of points.
\citeauthor{grotzbach1983} recognised that the above reasoning was only valid
for the mean grid spacing; large gradients in temperature and velocity near the
top and bottom plates require finer resolution in those regions. The notion of
nearness can be formalised using the thermal boundary layer thickness
\cref{eqn:thermal_bl}, and one asks how many grid points are necessary in this
layer. \citeauthor{grotzbach1983} did not give a theoretical argument to derive
this number but claimed that 3 points are sufficient for turbulent flows.
\textcite{shishkina2010} presented a theoretical argument based on the
(experimentally and numerically justified) assumption of laminar
\emph{Prandtl-Blasius} flow conditions in the boundary layer and were able to
calculate the minimum number of grid points (e.g., 9 for $\rayleigh =
\SI{2e9}{}$ and $\prandtl = 0.7$). The results agreed with criteria derived in
previous numerical experiments. Importantly, the results of
\citeauthor{shishkina2010} allow \emph{a priori} determination of vertical
resolution requirements, potentially bypassing the time-consuming and expensive
process of iteratively running simulations, checking their convergence and
updating the resolution.

\subsubsection{
    Resolution-dependence tests and consequences of under-resolution}%
\label{sec:res_tests}

Performing numerical experiments for a 3D fluid layer,
\citeauthor{grotzbach1983} found that RMS velocity and Nusselt number were
the most sensitive quantities to insufficient mean grid spacing, but even
they increased ``only slightly'' above the values obtained from well-resolved
simulations. He concluded that condition \cref{eqn:grotzbach} was overly
restrictive and recommended (for $\prandtl > 0.59$) the simplified,
approximate version
\[ % is this necessary?
    h \lesssim 5.23 \, \prandtl^{-1/4} \rayleigh^{-0.3205}.
\]
Later work also supports the finding that the Nusselt number is
sensitive to under-resolution. Even studying only steady-state convective
solutions at moderate Rayleigh number, \textcite{le_quere1991} found that the
maximum and minimum Nusselt numbers were most sensitive to changes in
resolution and had the largest uncertainty among existing benchmark solutions.
% ouertatani: common to use Nu as convergence test
Other studies have used the convergence of the Nusselt number as an indicator
that the spatial resolution is sufficient to produce an accurate solution
\parencite{ouertatani2008}.

\textcite{stevens2010} performed 3D simulations in a finite cylindrical cavity
with the aim of reconciling the apparent disagreement between the Nusselt
numbers in previous numerical studies and experimental observations. They found
that agreement with experiment could be achieved, but only by using a much
higher resolution than the previous studies. They offered the physical
explanation that horizontally under-resolved simulations produce insufficient
thermal diffusion, leading to systematic overestimation of the buoyancy of
convective plumes near the side-walls of the cylinder; this results in Nusselt
numbers that exceed experimentally observed values. This led them to conclude
that the two criteria of \textcite{grotzbach1983}---for mean grid spacing
and for the vertical spacing near the upper and lower plates---are not
independent; the definition $h = (\Delta x \Delta y \Delta z)^{1/3}$ in
\cref{eqn:grotzbach} allows the horizontal spacing to remain relatively
coarse near the plates, provided the vertical spacing is small. Since
fine horizontal resolution is also necessary to accurately capture
the dynamics of the thin plumes, they proposed that \cref{eqn:grotzbach} be
applied with $h = \max(\Delta x, \Delta y, \Delta z)$ instead.

% kooij: Nu may not be best indicator
Some more recent work, however, casts doubt on the notion that the Nusselt
number is sensitive to under-resolution and that its convergence is a good
indicator that the flow is well-resolved. In assessing the performance of
several published computational fluid dynamics codes on the \rb{} problem in a
cylindrical cavity, \textcite{kooij2018} identified one higher-order code that
reproduced the theoretically predicted scaling of $\nusselt$ as a function of
$\rayleigh$ even when the flow was deliberately under-resolved. On the other
hand, the presence of numerical artefacts in the instantaneous temperature
field near the bottom plate was a clear indicator of insufficient resolution.
\cref{fig:kooij2018_artefacts}, a reproduction of part of their Figure 5, shows
these artefacts.

% scheel: dissipation rates sensitive
\textcite{scheel2013} performed similar high-resolution simulations for
a cylindrical cavity and also found that the Nusselt number, among other
global transport properties, were ``fairly insensitive to insufficient
resolution, as long as the mean Kolmogorov length [was] resolved'' (i.e.,
\cref{eqn:grotzbach} was satisfied). However, they found that the horizontally
averaged or local kinetic energy dissipation rate
\cref{eqn:kinetic_dissipation} and the corresponding thermal dissipation rate
\begin{equation}
    \label{eqn:thermal_dissipation}
    \epsilon_T(\vec{x}, t) \equiv \kappa \sum_i \left(\pdiff{T}{x_i}\right)^2
\end{equation}
were much more sensitive, with their convergence requiring even stricter
conditions than \cref{eqn:grotzbach}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/kooij2018_artefacts.pdf}
    \caption{
        Reproduction of part of Figure 5 by \textcite{kooij2018}, showing an
        instantaneous horizontal temperature profile simulated by the
        \textsc{Nek}5000 code for \rb{} convection at $\rayleigh =
        \SI{e10}{}$. The profile is taken near the bottom plate of a
        cylindrical cavity and shows a grid-like pattern of numerical
        artefacts. The right panel is an enlargement of the region inside
        the black square in the left panel.
    }
    \label{fig:kooij2018_artefacts}
\end{figure}

\newpage
\section{Conclusion}

\newpage
\emergencystretch=5em
\printbibliography
\end{document}
